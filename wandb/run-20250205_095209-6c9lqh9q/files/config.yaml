_wandb:
    value:
        cli_version: 0.19.3
        m: []
        python_version: 3.11.9
        t:
            "1":
                - 1
                - 11
                - 49
                - 51
                - 55
                - 71
            "2":
                - 1
                - 11
                - 49
                - 51
                - 55
                - 71
            "3":
                - 13
                - 14
                - 16
                - 23
                - 55
            "4": 3.11.9
            "5": 0.19.3
            "6": 4.48.0
            "8":
                - 3
                - 5
            "12": 0.19.3
            "13": windows-amd64
adversary_dist_types:
    value: pile-bio:0.33,camel-bio:0.33,retain_forget_switch:0.33
adversary_lr_samples:
    value: 2e-6,2e-5,4e-5
adversary_lr_schedulers:
    value: constant:1.0
base:
    value: distilgpt2
base_model_name:
    value: distilgpt2
batch_size:
    value: 2
concept_data_split:
    value: 0.2
expname:
    value: latest
gradient_accumulation_steps:
    value: 8
inner_optimizer_warmup_steps:
    value: 20
lr:
    value: 2e-05
max_data_size:
    value: 40000
max_steps:
    value: 750
new_model_name:
    value: tar_model
retain_model_name:
    value: distilgpt2
retain_representations:
    value: false
retain_same_base:
    value: false
schedule_lambda:
    value: 0.5
subject:
    value: bio-multi-dists
switching_point_coeffs:
    value: alpha:6.0,beta:3.0
tar_adversary_batch_size:
    value: 1
tar_inner_loop_steps:
    value: 1
tar_inner_loop_subsample:
    value: 1
tar_num_tasks_sampled:
    value: 1
tar_retain_scale:
    value: 1
tar_tamper_resistance_grad_scale:
    value: 4
tar_tamper_resistance_loss_lower_bound:
    value: -11.76
tar_tamper_resistance_loss_type:
    value: max_entropy
trainer_type:
    value: tar_trainer
unbounded:
    value: false
use_weighting_schedule:
    value: false
wandb:
    value: true
wandb_project_name:
    value: tar_training
warmup_steps:
    value: 50
