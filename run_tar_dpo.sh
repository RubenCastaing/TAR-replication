accelerate launch --config_file $ExACCEL_CONFIG tar.py \
--trainer_type tar_trainer \
--max_steps 100 \
--tar_num_tasks_sampled 2 \
--max_data_size 256 \
--tar_tamper_resistance_loss_type dpo \
--tar_inner_loop_steps 8 \
--unbounded \
--use_weighting_schedule \
--tar_tamper_resistance_grad_scale 0.1 \
--tar_retain_scale 1.0 \
--schedule_lambda 0.0625 \
--warmup_steps 32 \
--lr 6e-05 \
--adversary_lr_samples 2e-6,2e-5,4e-5 \
--batch_size 4 \
--gradient_accumulation_steps 2 \
--adversary_dist_types harmful_completions:1.0 \
--adversary_lr_schedulers constant:1.0 \
--inner_optimizer_warmup_steps 20 \
--tar_inner_loop_subsample 1 \
--tar_inner_loop_batch_size 1 \
--base_model_name meta-llama/Meta-Llama-3-8B-Instruct \
--subject dpo_anthropic \
--base llama3 \
--new_model_name tar_llama3_dpo